{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62b9878",
   "metadata": {},
   "source": [
    "# Group Assignment Preparation\n",
    "Unpack the file \"IS453 Group Assignment - Data.zip\"<BR>\n",
    "Based on the data files and data dictionary, answer the questions below.\n",
    "\n",
    "**DIY Q4: Explore Assignment Data**\n",
    "- How many rows and column in each data file?\n",
    "- Through which variable is the data in two files linked?\n",
    "- Is the relationship between the records in the files one-to-one or one-to-many? In which direction?\n",
    "- Are all of the records in each file linked to the other? If not, which file has unlinked records?\n",
    "- Which variables would potentially conflict with fair lending principles?\n",
    "\n",
    "Application Data.csv: 307511 rows, 120 columns\n",
    "Bureau Data.csv: 1716428 rows, 17 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e8346f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/anaconda3/envs/is453/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/envs/is453/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7a60223",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_df = pd.read_csv('IS453 Group Assignment - Application Data.csv')\n",
    "bureau_df = pd.read_csv('IS453 Group Assignment - Bureau Data.csv')\n",
    "data_df = pd.read_excel('IS453 Group Assignment - Data Dict.xlsx', sheet_name=['Application Data', 'Bureau Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a208131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Data: 307511 rows, 120 columns\n",
      "Bureau Data: 1716428 rows, 17 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"Application Data: {app_df.shape[0]} rows, {app_df.shape[1]} columns\")\n",
    "print(f\"Bureau Data: {bureau_df.shape[0]} rows, {bureau_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e9a92c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns: {'SK_ID_CURR', 'AMT_ANNUITY'}\n"
     ]
    }
   ],
   "source": [
    "common_cols = set(app_df.columns) & set(bureau_df.columns)\n",
    "print(\"Common columns:\", common_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da8c663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique SK_ID_CURR in app_df: 307511\n",
      "Total rows in app_df: 307511\n",
      "Duplicates in app_df: 0\n",
      "\n",
      "Unique SK_ID_CURR in bureau_df: 305811\n",
      "Total rows in bureau_df: 1716428\n",
      "Duplicates in bureau_df: 1410617\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique SK_ID_CURR in app_df: {app_df['SK_ID_CURR'].nunique()}\")\n",
    "print(f\"Total rows in app_df: {len(app_df)}\")\n",
    "print(f\"Duplicates in app_df: {app_df['SK_ID_CURR'].duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nUnique SK_ID_CURR in bureau_df: {bureau_df['SK_ID_CURR'].nunique()}\")\n",
    "print(f\"Total rows in bureau_df: {len(bureau_df)}\")\n",
    "print(f\"Duplicates in bureau_df: {bureau_df['SK_ID_CURR'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ec02583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs only in app_df: 44020\n",
      "IDs only in bureau_df: 42320\n",
      "IDs in both: 263491\n"
     ]
    }
   ],
   "source": [
    "app_ids = set(app_df['SK_ID_CURR'])\n",
    "bureau_ids = set(bureau_df['SK_ID_CURR'])\n",
    "\n",
    "print(f\"IDs only in app_df: {len(app_ids - bureau_ids)}\")\n",
    "print(f\"IDs only in bureau_df: {len(bureau_ids - app_ids)}\")\n",
    "print(f\"IDs in both: {len(app_ids & bureau_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6deb79e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially sensitive variables:\n",
      "  - CODE_GENDER\n",
      "  - CNT_CHILDREN\n",
      "  - NAME_FAMILY_STATUS\n",
      "  - DAYS_BIRTH\n",
      "  - OWN_CAR_AGE\n"
     ]
    }
   ],
   "source": [
    "# Check column names for potentially sensitive variables\n",
    "sensitive_keywords = ['GENDER', 'AGE', 'BIRTH', 'FAMILY', 'CHILDREN', 'RACE', 'RELIGION', 'ETHNICITY']\n",
    "\n",
    "potentially_sensitive = [col for col in app_df.columns if any(keyword in col.upper() for keyword in sensitive_keywords)]\n",
    "print(\"Potentially sensitive variables:\")\n",
    "for col in potentially_sensitive:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ac3531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Application Data':                             Row  \\\n",
      "0                    SK_ID_CURR   \n",
      "1                        TARGET   \n",
      "2            NAME_CONTRACT_TYPE   \n",
      "3                   CODE_GENDER   \n",
      "4                  FLAG_OWN_CAR   \n",
      "..                          ...   \n",
      "115   AMT_REQ_CREDIT_BUREAU_DAY   \n",
      "116  AMT_REQ_CREDIT_BUREAU_WEEK   \n",
      "117   AMT_REQ_CREDIT_BUREAU_MON   \n",
      "118   AMT_REQ_CREDIT_BUREAU_QRT   \n",
      "119  AMT_REQ_CREDIT_BUREAU_YEAR   \n",
      "\n",
      "                                           Description Special  \n",
      "0                             ID of loan in our sample     NaN  \n",
      "1    Target variable (1 - client with payment diffi...     NaN  \n",
      "2          Identification if loan is cash or revolving     NaN  \n",
      "3                                 Gender of the client     NaN  \n",
      "4                        Flag if the client owns a car     NaN  \n",
      "..                                                 ...     ...  \n",
      "115  Number of enquiries to Credit Bureau about the...     NaN  \n",
      "116  Number of enquiries to Credit Bureau about the...     NaN  \n",
      "117  Number of enquiries to Credit Bureau about the...     NaN  \n",
      "118  Number of enquiries to Credit Bureau about the...     NaN  \n",
      "119  Number of enquiries to Credit Bureau about the...     NaN  \n",
      "\n",
      "[120 rows x 3 columns], 'Bureau Data':                        Row                                        Description  \\\n",
      "0               SK_ID_CURR  ID of loan in our sample - one loan in our sam...   \n",
      "1             SK_BUREAU_ID  Recoded ID of previous Credit Bureau credit re...   \n",
      "2            CREDIT_ACTIVE  Status of the Credit Bureau (CB) reported credits   \n",
      "3          CREDIT_CURRENCY       Recoded currency of the Credit Bureau credit   \n",
      "4              DAYS_CREDIT  How many days before current application did c...   \n",
      "5       CREDIT_DAY_OVERDUE  Number of days past due on CB credit at the ti...   \n",
      "6      DAYS_CREDIT_ENDDATE  Remaining duration of CB credit (in days) at t...   \n",
      "7        DAYS_ENDDATE_FACT  Days since CB credit ended at the time of appl...   \n",
      "8   AMT_CREDIT_MAX_OVERDUE  Maximal amount overdue on the Credit Bureau cr...   \n",
      "9       CNT_CREDIT_PROLONG  How many times was the Credit Bureau credit pr...   \n",
      "10          AMT_CREDIT_SUM  Current credit amount for the Credit Bureau cr...   \n",
      "11     AMT_CREDIT_SUM_DEBT               Current debt on Credit Bureau credit   \n",
      "12    AMT_CREDIT_SUM_LIMIT  Current credit limit of credit card reported i...   \n",
      "13  AMT_CREDIT_SUM_OVERDUE     Current amount overdue on Credit Bureau credit   \n",
      "14             CREDIT_TYPE       Type of Credit Bureau credit (Car, cash,...)   \n",
      "15      DAYS_CREDIT_UPDATE  How many days before loan application did last...   \n",
      "16             AMT_ANNUITY                Annuity of the Credit Bureau credit   \n",
      "\n",
      "    Special  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "2       NaN  \n",
      "3       NaN  \n",
      "4       NaN  \n",
      "5       NaN  \n",
      "6       NaN  \n",
      "7       NaN  \n",
      "8       NaN  \n",
      "9       NaN  \n",
      "10      NaN  \n",
      "11      NaN  \n",
      "12      NaN  \n",
      "13      NaN  \n",
      "14      NaN  \n",
      "15      NaN  \n",
      "16      NaN  }\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71c59998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>CREDIT_ACTIVE</th>\n",
       "      <th>CREDIT_CURRENCY</th>\n",
       "      <th>DAYS_CREDIT</th>\n",
       "      <th>CREDIT_DAY_OVERDUE</th>\n",
       "      <th>DAYS_CREDIT_ENDDATE</th>\n",
       "      <th>DAYS_ENDDATE_FACT</th>\n",
       "      <th>AMT_CREDIT_MAX_OVERDUE</th>\n",
       "      <th>CNT_CREDIT_PROLONG</th>\n",
       "      <th>AMT_CREDIT_SUM</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT</th>\n",
       "      <th>AMT_CREDIT_SUM_LIMIT</th>\n",
       "      <th>AMT_CREDIT_SUM_OVERDUE</th>\n",
       "      <th>CREDIT_TYPE</th>\n",
       "      <th>DAYS_CREDIT_UPDATE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714462</td>\n",
       "      <td>Closed</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-497</td>\n",
       "      <td>0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>91323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714463</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-208</td>\n",
       "      <td>0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>171342.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714464</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>464323.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714465</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714466</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-629</td>\n",
       "      <td>0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77674.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2700000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY  DAYS_CREDIT  \\\n",
       "0      215354       5714462        Closed      currency 1         -497   \n",
       "1      215354       5714463        Active      currency 1         -208   \n",
       "2      215354       5714464        Active      currency 1         -203   \n",
       "3      215354       5714465        Active      currency 1         -203   \n",
       "4      215354       5714466        Active      currency 1         -629   \n",
       "\n",
       "   CREDIT_DAY_OVERDUE  DAYS_CREDIT_ENDDATE  DAYS_ENDDATE_FACT  \\\n",
       "0                   0               -153.0             -153.0   \n",
       "1                   0               1075.0                NaN   \n",
       "2                   0                528.0                NaN   \n",
       "3                   0                  NaN                NaN   \n",
       "4                   0               1197.0                NaN   \n",
       "\n",
       "   AMT_CREDIT_MAX_OVERDUE  CNT_CREDIT_PROLONG  AMT_CREDIT_SUM  \\\n",
       "0                     NaN                   0         91323.0   \n",
       "1                     NaN                   0        225000.0   \n",
       "2                     NaN                   0        464323.5   \n",
       "3                     NaN                   0         90000.0   \n",
       "4                 77674.5                   0       2700000.0   \n",
       "\n",
       "   AMT_CREDIT_SUM_DEBT  AMT_CREDIT_SUM_LIMIT  AMT_CREDIT_SUM_OVERDUE  \\\n",
       "0                  0.0                   NaN                     0.0   \n",
       "1             171342.0                   NaN                     0.0   \n",
       "2                  NaN                   NaN                     0.0   \n",
       "3                  NaN                   NaN                     0.0   \n",
       "4                  NaN                   NaN                     0.0   \n",
       "\n",
       "       CREDIT_TYPE  DAYS_CREDIT_UPDATE  AMT_ANNUITY  \n",
       "0  Consumer credit                -131          NaN  \n",
       "1      Credit card                 -20          NaN  \n",
       "2  Consumer credit                 -16          NaN  \n",
       "3      Credit card                 -16          NaN  \n",
       "4  Consumer credit                 -21          NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7e0c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Data Columns:\n",
      "['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE', 'EXT_SOURCE_1', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
      "\n",
      "Bureau Data Columns:\n",
      "['SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY']\n"
     ]
    }
   ],
   "source": [
    "# Application Data columns\n",
    "print(\"Application Data Columns:\")\n",
    "print(app_df.columns.tolist())\n",
    "\n",
    "# Bureau Data columns\n",
    "print(\"\\nBureau Data Columns:\")\n",
    "print(bureau_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4c9ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117000000.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app_df.head()\n",
    "app_df['AMT_INCOME_TOTAL'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71cd4ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_INCOME_TYPE\n",
       "Working                 158774\n",
       "Commercial associate     71617\n",
       "Pensioner                55362\n",
       "State servant            21703\n",
       "Unemployed                  22\n",
       "Student                     18\n",
       "Businessman                 10\n",
       "Maternity leave              5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_df['NAME_INCOME_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dcbf393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of applicants age: 156565\n",
      "Count of applicants income: 23229\n",
      "Count of applicants employment: 224233\n",
      "Count of applicants contract: NAME_CONTRACT_TYPE\n",
      "Cash loans    278232\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "age_count = app_df['DAYS_BIRTH'][[-16425 < x <= -9125 for x in app_df['DAYS_BIRTH']]].count()\n",
    "income_count = app_df['AMT_INCOME_TOTAL'][[36000 <= x <= 72000 for x in app_df['AMT_INCOME_TOTAL']]].count()\n",
    "employment_count = app_df['DAYS_EMPLOYED'][app_df['DAYS_EMPLOYED'] <= -365].count()\n",
    "contract_count = app_df['NAME_CONTRACT_TYPE'][app_df['NAME_CONTRACT_TYPE'] == 'Cash loans'].value_counts()\n",
    "\n",
    "print(f\"Count of applicants age: {age_count}\")\n",
    "print(f\"Count of applicants income: {income_count}\")\n",
    "print(f\"Count of applicants employment: {employment_count}\")\n",
    "print(f\"Count of applicants contract: {contract_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "809b6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY CHECK - APPLICATION DATA\n",
      "================================================================================\n",
      "\n",
      "1. MISSING VALUES IN KEY FILTERING COLUMNS:\n",
      "  SK_ID_CURR: 0 missing (0.00%)\n",
      "  DAYS_BIRTH: 0 missing (0.00%)\n",
      "  DAYS_EMPLOYED: 0 missing (0.00%)\n",
      "  AMT_INCOME_TOTAL: 0 missing (0.00%)\n",
      "  NAME_INCOME_TYPE: 0 missing (0.00%)\n",
      "  NAME_CONTRACT_TYPE: 0 missing (0.00%)\n",
      "  TARGET: 0 missing (0.00%)\n",
      "\n",
      "2. DUPLICATE CHECK:\n",
      "  Duplicate SK_ID_CURR in app_df: 0\n",
      "\n",
      "3. OUTLIER/INVALID VALUE CHECK:\n",
      "\n",
      "  DAYS_BIRTH (Age):\n",
      "    Min: -25229 (Age: 69.1 years)\n",
      "    Max: -7489 (Age: 20.5 years)\n",
      "    Invalid/Extreme values: 0\n",
      "\n",
      "  DAYS_EMPLOYED:\n",
      "    Min: -17912\n",
      "    Max: 365243\n",
      "    Suspicious values: 55374\n",
      "    Placeholder values (365243): 55374\n",
      "\n",
      "  AMT_INCOME_TOTAL:\n",
      "    Min: 25650.0\n",
      "    Max: 117000000.0\n",
      "    Mean: 168797.92\n",
      "    Median: 147150.00\n",
      "    Zero/Negative income: 0\n",
      "\n",
      "  TARGET (Default Indicator):\n",
      "    Value counts:\n",
      "TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n",
      "    Default rate: 8.07%\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECK - BUREAU DATA\n",
      "================================================================================\n",
      "\n",
      "4. BUREAU DATA STRUCTURE:\n",
      "  Total records: 1716428\n",
      "  Unique customers: 305811\n",
      "  Average records per customer: 5.61\n",
      "  Missing SK_ID_CURR: 0\n",
      "\n",
      "5. MISSING VALUES IN BUREAU KEY COLUMNS:\n",
      "  SK_ID_CURR: 0 (0.00%)\n",
      "  SK_ID_BUREAU: 0 (0.00%)\n",
      "  CREDIT_ACTIVE: 0 (0.00%)\n",
      "  CREDIT_TYPE: 0 (0.00%)\n",
      "  DAYS_CREDIT: 0 (0.00%)\n",
      "  AMT_CREDIT_SUM: 13 (0.00%)\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Based on the checks above:\n",
      "\n",
      "1. CRITICAL ISSUES TO ADDRESS BEFORE MERGING:\n",
      "   ⚠ 55374 records have placeholder DAYS_EMPLOYED value (365243)\n",
      "      Consider: Treat as missing or create separate category\n",
      "\n",
      "2. RECOMMENDED ACTIONS BEFORE FILTERING:\n",
      "   • Keep all records for now - filtering will handle data quality\n",
      "   • Document any known data quality issues\n",
      "   • Decide on bureau aggregation strategy\n",
      "\n",
      "3. DATA CLEANING TO DO AFTER MERGING:\n",
      "   • Handle missing values systematically\n",
      "   • Treat outliers using z-score method or domain knowledge\n",
      "   • Create engineered features from bureau data\n",
      "   • Perform feature selection for modeling\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your datasets (assuming you already have them loaded)\n",
    "# app_df = pd.read_csv('IS453 Group Assignment - Application Data.csv')\n",
    "# bureau_df = pd.read_csv('IS453 Group Assignment - Bureau Data.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY CHECK - APPLICATION DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Check for missing values in KEY columns needed for filtering\n",
    "print(\"\\n1. MISSING VALUES IN KEY FILTERING COLUMNS:\")\n",
    "key_columns = ['SK_ID_CURR', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'AMT_INCOME_TOTAL', \n",
    "               'NAME_INCOME_TYPE', 'NAME_CONTRACT_TYPE', 'TARGET']\n",
    "\n",
    "missing_summary = {}\n",
    "for col in key_columns:\n",
    "    if col in app_df.columns:\n",
    "        missing_count = app_df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(app_df)) * 100\n",
    "        missing_summary[col] = {\n",
    "            'Missing Count': missing_count,\n",
    "            'Missing %': f\"{missing_pct:.2f}%\"\n",
    "        }\n",
    "        print(f\"  {col}: {missing_count} missing ({missing_pct:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col}: COLUMN NOT FOUND\")\n",
    "\n",
    "# 2. Check for duplicates in SK_ID_CURR\n",
    "print(\"\\n2. DUPLICATE CHECK:\")\n",
    "duplicates = app_df['SK_ID_CURR'].duplicated().sum()\n",
    "print(f\"  Duplicate SK_ID_CURR in app_df: {duplicates}\")\n",
    "\n",
    "# 3. Check for invalid/outlier values in filtering columns\n",
    "print(\"\\n3. OUTLIER/INVALID VALUE CHECK:\")\n",
    "\n",
    "# Check DAYS_BIRTH (should be negative, typically between -7300 and -36500)\n",
    "print(\"\\n  DAYS_BIRTH (Age):\")\n",
    "print(f\"    Min: {app_df['DAYS_BIRTH'].min()} (Age: {abs(app_df['DAYS_BIRTH'].min())/365:.1f} years)\")\n",
    "print(f\"    Max: {app_df['DAYS_BIRTH'].max()} (Age: {abs(app_df['DAYS_BIRTH'].max())/365:.1f} years)\")\n",
    "invalid_birth = app_df[(app_df['DAYS_BIRTH'] > 0) | (app_df['DAYS_BIRTH'] < -36500)]\n",
    "print(f\"    Invalid/Extreme values: {len(invalid_birth)}\")\n",
    "\n",
    "# Check DAYS_EMPLOYED\n",
    "print(\"\\n  DAYS_EMPLOYED:\")\n",
    "print(f\"    Min: {app_df['DAYS_EMPLOYED'].min()}\")\n",
    "print(f\"    Max: {app_df['DAYS_EMPLOYED'].max()}\")\n",
    "# Check for positive values (invalid) or extremely high values\n",
    "invalid_employed = app_df[(app_df['DAYS_EMPLOYED'] > 0) | (app_df['DAYS_EMPLOYED'] < -18250)]\n",
    "print(f\"    Suspicious values: {len(invalid_employed)}\")\n",
    "# Check for the common placeholder value 365243\n",
    "placeholder_employed = (app_df['DAYS_EMPLOYED'] == 365243).sum()\n",
    "print(f\"    Placeholder values (365243): {placeholder_employed}\")\n",
    "\n",
    "# Check AMT_INCOME_TOTAL\n",
    "print(\"\\n  AMT_INCOME_TOTAL:\")\n",
    "print(f\"    Min: {app_df['AMT_INCOME_TOTAL'].min()}\")\n",
    "print(f\"    Max: {app_df['AMT_INCOME_TOTAL'].max()}\")\n",
    "print(f\"    Mean: {app_df['AMT_INCOME_TOTAL'].mean():.2f}\")\n",
    "print(f\"    Median: {app_df['AMT_INCOME_TOTAL'].median():.2f}\")\n",
    "# Check for zero or negative income\n",
    "invalid_income = app_df[app_df['AMT_INCOME_TOTAL'] <= 0]\n",
    "print(f\"    Zero/Negative income: {len(invalid_income)}\")\n",
    "\n",
    "# Check TARGET variable\n",
    "print(\"\\n  TARGET (Default Indicator):\")\n",
    "print(f\"    Value counts:\\n{app_df['TARGET'].value_counts()}\")\n",
    "print(f\"    Default rate: {app_df['TARGET'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECK - BUREAU DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4. Check bureau data\n",
    "print(\"\\n4. BUREAU DATA STRUCTURE:\")\n",
    "print(f\"  Total records: {len(bureau_df)}\")\n",
    "print(f\"  Unique customers: {bureau_df['SK_ID_CURR'].nunique()}\")\n",
    "print(f\"  Average records per customer: {len(bureau_df)/bureau_df['SK_ID_CURR'].nunique():.2f}\")\n",
    "\n",
    "# Check for missing SK_ID_CURR\n",
    "missing_ids = bureau_df['SK_ID_CURR'].isna().sum()\n",
    "print(f\"  Missing SK_ID_CURR: {missing_ids}\")\n",
    "\n",
    "# Check key bureau columns for missing values\n",
    "print(\"\\n5. MISSING VALUES IN BUREAU KEY COLUMNS:\")\n",
    "bureau_key_cols = ['SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_TYPE', \n",
    "                   'DAYS_CREDIT', 'AMT_CREDIT_SUM']\n",
    "for col in bureau_key_cols:\n",
    "    if col in bureau_df.columns:\n",
    "        missing = bureau_df[col].isna().sum()\n",
    "        missing_pct = (missing / len(bureau_df)) * 100\n",
    "        print(f\"  {col}: {missing} ({missing_pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBased on the checks above:\")\n",
    "print(\"\\n1. CRITICAL ISSUES TO ADDRESS BEFORE MERGING:\")\n",
    "\n",
    "# Check if any critical columns have issues\n",
    "critical_issues = []\n",
    "\n",
    "if missing_summary.get('SK_ID_CURR', {}).get('Missing Count', 0) > 0:\n",
    "    critical_issues.append(\"   ⚠ SK_ID_CURR has missing values - these records cannot be merged\")\n",
    "\n",
    "if missing_summary.get('TARGET', {}).get('Missing Count', 0) > 0:\n",
    "    critical_issues.append(\"   ⚠ TARGET has missing values - cannot calculate bad rates accurately\")\n",
    "\n",
    "if placeholder_employed > 0:\n",
    "    critical_issues.append(f\"   ⚠ {placeholder_employed} records have placeholder DAYS_EMPLOYED value (365243)\")\n",
    "    critical_issues.append(\"      Consider: Treat as missing or create separate category\")\n",
    "\n",
    "if len(invalid_income) > 0:\n",
    "    critical_issues.append(f\"   ⚠ {len(invalid_income)} records have zero/negative income\")\n",
    "\n",
    "if len(critical_issues) > 0:\n",
    "    for issue in critical_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"   ✓ No critical issues found in key merge columns\")\n",
    "\n",
    "print(\"\\n2. RECOMMENDED ACTIONS BEFORE FILTERING:\")\n",
    "print(\"   • Keep all records for now - filtering will handle data quality\")\n",
    "print(\"   • Document any known data quality issues\")\n",
    "print(\"   • Decide on bureau aggregation strategy\")\n",
    "\n",
    "print(\"\\n3. DATA CLEANING TO DO AFTER MERGING:\")\n",
    "print(\"   • Handle missing values systematically\")\n",
    "print(\"   • Treat outliers using z-score method or domain knowledge\")\n",
    "print(\"   • Create engineered features from bureau data\")\n",
    "print(\"   • Perform feature selection for modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c51be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APPLYING DATA SELECTION CRITERIA FOR LIFELONG LEARNING LOANS\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL DATASET:\n",
      "  Total records: 307,511\n",
      "  Goods (TARGET=0): 282,686\n",
      "  Bads (TARGET=1): 24,825\n",
      "  Bad rate: 8.07%\n",
      "\n",
      "================================================================================\n",
      "APPLYING 5 FILTERING CRITERIA\n",
      "================================================================================\n",
      "\n",
      "FILTERED DATASET (Target Segment):\n",
      "  Total records: 31,125\n",
      "  Goods (TARGET=0): 28,016\n",
      "  Bads (TARGET=1): 3,109\n",
      "  Bad rate: 9.99%\n",
      "\n",
      "================================================================================\n",
      "COMPARISON METRICS\n",
      "================================================================================\n",
      "\n",
      "RETENTION:\n",
      "  Records retained: 31,125 out of 307,511\n",
      "  Retention rate: 10.12%\n",
      "\n",
      "BAD RATE COMPARISON:\n",
      "  Original bad rate: 8.07%\n",
      "  Filtered bad rate: 9.99%\n",
      "  Difference: +1.92 percentage points\n",
      "  → Target segment is HIGHER RISK (worse than average)\n",
      "\n",
      "================================================================================\n",
      "BREAKDOWN BY INDIVIDUAL CRITERIA\n",
      "================================================================================\n",
      "\n",
      "How each criterion narrows the dataset:\n",
      "\n",
      "1. Age (25-55 years)\n",
      "  Records: 226,662 (73.71%)\n",
      "  Bad rate: 8.71%\n",
      "\n",
      "4. Income Type (Working/Commercial/State)\n",
      "  Records: 252,116 (81.99%)\n",
      "  Bad rate: 8.66%\n",
      "\n",
      "5. Contract Type (Cash loans)\n",
      "  Records: 278,232 (90.48%)\n",
      "  Bad rate: 8.35%\n",
      "\n",
      "DATA SELECTION RESULTS:\n",
      "\n",
      "1. ORIGINAL DATASET SIZE: 307,511 applicants\n",
      "\n",
      "2. FILTERED DATASET SIZE: 31,125 applicants\n",
      "   - Retention rate: 10.12%\n",
      "\n",
      "3. BAD RATE COMPARISON:\n",
      "   - Original dataset: 8.07%\n",
      "   - Target segment: 9.99%\n",
      "   - Difference: +1.92 percentage points\n",
      "\n",
      "4. INTERPRETATION:\n",
      "   The target segment for lifelong learning loans (mid-career professionals\n",
      "   aged 25-45 with stable employment and middle income) represents \n",
      "   10.12% of the original dataset and shows a \n",
      "   HIGHER \n",
      "   default risk compared to the overall portfolio.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have already loaded:\n",
    "# app_df = pd.read_csv('IS453 Group Assignment - Application Data.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING DATA SELECTION CRITERIA FOR LIFELONG LEARNING LOANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store original dataset metrics\n",
    "total_original = len(app_df)\n",
    "original_bad_count = app_df['TARGET'].sum()\n",
    "original_bad_rate = app_df['TARGET'].mean() * 100\n",
    "\n",
    "print(f\"\\nORIGINAL DATASET:\")\n",
    "print(f\"  Total records: {total_original:,}\")\n",
    "print(f\"  Goods (TARGET=0): {(app_df['TARGET']==0).sum():,}\")\n",
    "print(f\"  Bads (TARGET=1): {original_bad_count:,}\")\n",
    "print(f\"  Bad rate: {original_bad_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING 5 FILTERING CRITERIA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply all 5 criteria simultaneously\n",
    "filtered_df = app_df[\n",
    "    (app_df['DAYS_BIRTH'] >= -20075) & \n",
    "    (app_df['DAYS_BIRTH'] <= -9125) &\n",
    "    # (app_df['AMT_INCOME_TOTAL'] >= 0) & \n",
    "    (app_df['AMT_INCOME_TOTAL'] <= 96000) &\n",
    "    # (app_df['DAYS_EMPLOYED'] <= -365) &\n",
    "    (app_df['NAME_CONTRACT_TYPE'] == 'Cash loans') &\n",
    "    (app_df['NAME_INCOME_TYPE'].isin(['Working', 'Commercial associate', 'State servant','Unemployed','Maternity Leave']))\n",
    "]\n",
    "\n",
    "# Calculate filtered dataset metrics\n",
    "total_filtered = len(filtered_df)\n",
    "filtered_bad_count = filtered_df['TARGET'].sum()\n",
    "filtered_bad_rate = filtered_df['TARGET'].mean() * 100\n",
    "\n",
    "percentage_retained = (total_filtered / total_original) * 100\n",
    "\n",
    "print(\"\\nFILTERED DATASET (Target Segment):\")\n",
    "print(f\"  Total records: {total_filtered:,}\")\n",
    "print(f\"  Goods (TARGET=0): {(filtered_df['TARGET']==0).sum():,}\")\n",
    "print(f\"  Bads (TARGET=1): {filtered_bad_count:,}\")\n",
    "print(f\"  Bad rate: {filtered_bad_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate differences\n",
    "bad_rate_difference = filtered_bad_rate - original_bad_rate\n",
    "retention_rate = percentage_retained\n",
    "\n",
    "print(f\"\\nRETENTION:\")\n",
    "print(f\"  Records retained: {total_filtered:,} out of {total_original:,}\")\n",
    "print(f\"  Retention rate: {retention_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nBAD RATE COMPARISON:\")\n",
    "print(f\"  Original bad rate: {original_bad_rate:.2f}%\")\n",
    "print(f\"  Filtered bad rate: {filtered_bad_rate:.2f}%\")\n",
    "print(f\"  Difference: {bad_rate_difference:+.2f} percentage points\")\n",
    "\n",
    "if bad_rate_difference < 0:\n",
    "    print(f\"  → Target segment is LOWER RISK (better than average)\")\n",
    "elif bad_rate_difference > 0:\n",
    "    print(f\"  → Target segment is HIGHER RISK (worse than average)\")\n",
    "else:\n",
    "    print(f\"  → Target segment has SIMILAR RISK to overall portfolio\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BREAKDOWN BY INDIVIDUAL CRITERIA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show impact of each filter\n",
    "print(\"\\nHow each criterion narrows the dataset:\")\n",
    "\n",
    "criteria = [\n",
    "    ('1. Age (25-55 years)', \n",
    "     app_df[(app_df['DAYS_BIRTH'] >= -20075) & (app_df['DAYS_BIRTH'] <= -9125)]),\n",
    "    # ('2. Employment (1+ years)', \n",
    "    #  app_df[app_df['DAYS_EMPLOYED'] <= -365]),\n",
    "    # ('3. Income (36K-72K)', \n",
    "    #  app_df[(app_df['AMT_INCOME_TOTAL'] >= 36000) & (app_df['AMT_INCOME_TOTAL'] <= 72000)]),\n",
    "    ('4. Income Type (Working/Commercial/State)', \n",
    "     app_df[app_df['NAME_INCOME_TYPE'].isin(['Working', 'Commercial associate', 'State servant','Unemployed', 'Maternity Leave'])]),\n",
    "    ('5. Contract Type (Cash loans)', \n",
    "     app_df[app_df['NAME_CONTRACT_TYPE'] == 'Cash loans'])\n",
    "]\n",
    "\n",
    "for name, subset in criteria:\n",
    "    count = len(subset)\n",
    "    pct = (count / total_original) * 100\n",
    "    bad_rate = subset['TARGET'].mean() * 100\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Records: {count:,} ({pct:.2f}%)\")\n",
    "    print(f\"  Bad rate: {bad_rate:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA SELECTION RESULTS:\n",
    "\n",
    "1. ORIGINAL DATASET SIZE: {total_original:,} applicants\n",
    "\n",
    "2. FILTERED DATASET SIZE: {total_filtered:,} applicants\n",
    "   - Retention rate: {retention_rate:.2f}%\n",
    "\n",
    "3. BAD RATE COMPARISON:\n",
    "   - Original dataset: {original_bad_rate:.2f}%\n",
    "   - Target segment: {filtered_bad_rate:.2f}%\n",
    "   - Difference: {bad_rate_difference:+.2f} percentage points\n",
    "\n",
    "4. INTERPRETATION:\n",
    "   The target segment for lifelong learning loans (mid-career professionals\n",
    "   aged 25-45 with stable employment and middle income) represents \n",
    "   {retention_rate:.2f}% of the original dataset and shows a \n",
    "   {\"LOWER\" if bad_rate_difference < 0 else \"HIGHER\" if bad_rate_difference > 0 else \"SIMILAR\"} \n",
    "   default risk compared to the overall portfolio.\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaa97f",
   "metadata": {},
   "source": [
    "## Flattening Bureau Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82f38d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "⚙️  Skipping flattening pipeline: 'IS453_Group_Assignment_Bureau_Flattened.csv' already exists.\n",
      "If you want to re-run the flattening process, delete the existing file first.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "output_file = \"IS453_Group_Assignment_Bureau_Flattened.csv\"\n",
    "\n",
    "# === EARLY EXIT GUARD ===\n",
    "if os.path.exists(output_file):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"⚙️  Skipping flattening pipeline: '{output_file}' already exists.\")\n",
    "    print(\"If you want to re-run the flattening process, delete the existing file first.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PHASE 1: LOAD AND VALIDATE BUREAU DATA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Bureau data should already be loaded as bureau_df\n",
    "    print(f\"\\nBureau Data loaded:\")\n",
    "    print(f\"  Shape: {bureau_df.shape}\")\n",
    "    print(f\"  Unique customers: {bureau_df['SK_ID_CURR'].nunique()}\")\n",
    "    print(f\"  Average records per customer: {len(bureau_df)/bureau_df['SK_ID_CURR'].nunique():.2f}\")\n",
    "\n",
    "    # Verify key columns exist\n",
    "    required_cols = ['SK_ID_CURR', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE',\n",
    "                    'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT',\n",
    "                    'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', \n",
    "                    'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE',\n",
    "                    'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY']\n",
    "\n",
    "    missing_cols = [col for col in required_cols if col not in bureau_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"  ⚠ Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All required columns present\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: AGGREGATE NUMERICAL COLUMNS (12 columns × 4 functions = 48 features)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Define numerical columns to aggregate\n",
    "    numerical_cols = [\n",
    "        'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT',\n",
    "        'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', \n",
    "        'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE',\n",
    "        'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY'\n",
    "    ]\n",
    "\n",
    "    # Group by SK_ID_CURR and aggregate\n",
    "    print(\"\\nAggregating numerical columns...\")\n",
    "    numerical_agg = bureau_df.groupby('SK_ID_CURR')[numerical_cols].agg(['min', 'max', 'mean', 'sum'])\n",
    "\n",
    "    # Flatten the multi-level column names\n",
    "    numerical_agg.columns = [f\"{col}_{func}\".upper() for col, func in numerical_agg.columns]\n",
    "\n",
    "    print(f\"  ✓ Created {len(numerical_agg.columns)} numerical aggregate columns\")\n",
    "    print(f\"  ✓ Result shape: {numerical_agg.shape}\")\n",
    "    print(f\"\\n  Sample numerical columns:\")\n",
    "    print(f\"    {list(numerical_agg.columns[:8])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: AGGREGATE CATEGORICAL COLUMNS (3 columns → counts)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Define categorical columns\n",
    "    categorical_cols = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE']\n",
    "\n",
    "    # Create categorical aggregations\n",
    "    categorical_agg_dict = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nProcessing {col}:\")\n",
    "        \n",
    "        # Get unique values for this column\n",
    "        unique_values = bureau_df[col].unique()\n",
    "        print(f\"  Unique values: {len(unique_values)}\")\n",
    "        print(f\"  Examples: {list(unique_values[:5])}\")\n",
    "        \n",
    "        # Count occurrences of each category for each customer\n",
    "        for category in unique_values:\n",
    "            col_name = f\"COUNT_{col}_{str(category).upper().replace(' ', '_')}\"\n",
    "            categorical_agg_dict[col_name] = (\n",
    "                bureau_df.groupby('SK_ID_CURR')[col].apply(lambda x: (x == category).sum())\n",
    "            )\n",
    "\n",
    "    # Create dataframe from categorical aggregations\n",
    "    categorical_agg = pd.DataFrame(categorical_agg_dict)\n",
    "\n",
    "    print(f\"\\n✓ Created {len(categorical_agg.columns)} categorical count columns\")\n",
    "    print(f\"✓ Result shape: {categorical_agg.shape}\")\n",
    "    print(f\"\\nSample categorical columns (first 10):\")\n",
    "    print(f\"  {list(categorical_agg.columns[:10])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 4: COMBINE ALL AGGREGATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Reset index to make SK_ID_CURR a column\n",
    "    numerical_agg_reset = numerical_agg.reset_index()\n",
    "    categorical_agg_reset = categorical_agg.reset_index()\n",
    "\n",
    "    # Merge numerical and categorical aggregations\n",
    "    bureau_flat = numerical_agg_reset.merge(categorical_agg_reset, on='SK_ID_CURR', how='inner')\n",
    "\n",
    "    print(f\"\\n✓ Merged datasets:\")\n",
    "    print(f\"  Rows: {len(bureau_flat)}\")\n",
    "    print(f\"  Columns: {len(bureau_flat.columns)}\")\n",
    "    print(f\"    - 1 SK_ID_CURR (identifier)\")\n",
    "    print(f\"    - {len(numerical_agg.columns)} numerical aggregates\")\n",
    "    print(f\"    - {len(categorical_agg.columns)} categorical counts\")\n",
    "    print(f\"  Total: {1 + len(numerical_agg.columns) + len(categorical_agg.columns)}\")\n",
    "\n",
    "    print(\"\\n✓ Final flattened dataset shape: {bureau_flat.shape}\")\n",
    "\n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst 3 rows of flattened data (sample columns):\")\n",
    "    sample_cols = ['SK_ID_CURR', 'DAYS_CREDIT_MIN', 'DAYS_CREDIT_MAX', 'DAYS_CREDIT_MEAN', \n",
    "                'AMT_CREDIT_SUM_SUM', 'CNT_CREDIT_PROLONG_MAX', 'COUNT_CREDIT_ACTIVE_ACTIVE', \n",
    "                'COUNT_CREDIT_TYPE_CONSUMER_CREDIT']\n",
    "    sample_cols_present = [col for col in sample_cols if col in bureau_flat.columns]\n",
    "    print(bureau_flat[sample_cols_present].head(3).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 5: VALIDATE FLATTENED OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Validation checks\n",
    "    print(\"\\n1. Uniqueness Check:\")\n",
    "    print(f\"   Unique SK_ID_CURR: {bureau_flat['SK_ID_CURR'].nunique()}\")\n",
    "    print(f\"   Total rows: {len(bureau_flat)}\")\n",
    "    print(f\"   ✓ No duplicates\" if bureau_flat['SK_ID_CURR'].nunique() == len(bureau_flat) else \"   ⚠ Duplicates found!\")\n",
    "\n",
    "    print(\"\\n2. Column Naming Check:\")\n",
    "    numerical_cols_count = sum(1 for col in bureau_flat.columns if '_MIN' in col or '_MAX' in col or '_MEAN' in col or '_SUM' in col)\n",
    "    count_cols_count = sum(1 for col in bureau_flat.columns if col.startswith('COUNT_'))\n",
    "    print(f\"   Numerical aggregate columns (with _MIN/_MAX/_MEAN/_SUM): {numerical_cols_count}\")\n",
    "    print(f\"   Categorical count columns (COUNT_*): {count_cols_count}\")\n",
    "    print(f\"   Total: {len(bureau_flat.columns)}\")\n",
    "\n",
    "    print(\"\\n3. Missing Values Check (categorical counts):\")\n",
    "    count_cols = [col for col in bureau_flat.columns if col.startswith('COUNT_')]\n",
    "    nan_in_counts = bureau_flat[count_cols].isna().sum().sum()\n",
    "    print(f\"   NaN values in COUNT_* columns: {nan_in_counts}\")\n",
    "    print(f\"   ✓ No NaN in categorical counts\" if nan_in_counts == 0 else \"   ⚠ Found NaN values!\")\n",
    "\n",
    "    print(\"\\n4. Data Value Check:\")\n",
    "    print(f\"   Min value across all aggregates: {bureau_flat.iloc[:, 1:].min().min():.2f}\")\n",
    "    print(f\"   Max value across all aggregates: {bureau_flat.iloc[:, 1:].max().max():.2f}\")\n",
    "    print(f\"   ✓ All values are numeric and non-negative (as expected)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 6: EXPORT TO CSV\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Export to CSV\n",
    "    output_file = 'IS453_Group_Assignment_Bureau_Flattened.csv'\n",
    "    bureau_flat.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Exported flattened Bureau data to: {output_file}\")\n",
    "    print(f\"  File size: {bureau_flat.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n📊 Dataset Transformation:\")\n",
    "    print(f\"  Original Bureau Data:   1,716,428 rows × 17 columns\")\n",
    "    print(f\"  Flattened Bureau Data:  {len(bureau_flat):,} rows × {len(bureau_flat.columns)} columns\")\n",
    "    print(f\"  Compression ratio:      {100 * len(bureau_flat) / 1716428:.2f}%\")\n",
    "\n",
    "    print(\"\\n📈 Feature Breakdown:\")\n",
    "    print(f\"  Identifier column:      1 (SK_ID_CURR)\")\n",
    "    print(f\"  Numerical aggregates:   {numerical_cols_count} (MIN, MAX, MEAN, SUM for 12 columns)\")\n",
    "    print(f\"  Categorical counts:     {count_cols_count} (one-hot encoded)\")\n",
    "    print(f\"  Total output columns:   {len(bureau_flat.columns)}\")\n",
    "\n",
    "    print(\"\\n✓ Bureau data flattening complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c8b1a",
   "metadata": {},
   "source": [
    "## Merge Application Data with Flattened Bureau Data\n",
    "\n",
    "This section demonstrates how to merge the two datasets using different join types and handle missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d7n7s0a0io",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LEFT JOIN: Application Data (LEFT) ← Bureau Data (RIGHT)\n",
      "================================================================================\n",
      "\n",
      "Dataset Configuration:\n",
      "  LEFT table (app_df):      307,511 rows × 120 columns\n",
      "  RIGHT table (bureau_flat): 305,811 rows × 72 columns\n",
      "  Join key: SK_ID_CURR\n",
      "  Join type: LEFT JOIN\n",
      "\n",
      "What a LEFT JOIN does:\n",
      "  ✓ Keeps ALL rows from the LEFT table (app_df)\n",
      "  ✓ Adds matching data from the RIGHT table (bureau_flat)\n",
      "  ✓ For non-matching rows: fills with NaN\n",
      "\n",
      "================================================================================\n",
      "EXECUTING LEFT JOIN\n",
      "================================================================================\n",
      "\n",
      "✓ Merge complete!\n",
      "\n",
      "Result Dataset:\n",
      "  Rows: 307,511\n",
      "  Columns: 191\n",
      "\n",
      "Column breakdown:\n",
      "  Application columns: 120\n",
      "  Bureau columns (from right): 71 (SK_ID_CURR not duplicated)\n",
      "  Total: 191\n",
      "\n",
      "================================================================================\n",
      "MERGE RESULT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Applicants with Bureau data:\n",
      "  Count: 263,491\n",
      "  Percentage: 85.69%\n",
      "\n",
      "Applicants WITHOUT Bureau data (new-to-credit):\n",
      "  Count: 44,020\n",
      "  Percentage: 14.31%\n",
      "  Status: Will have NaN values in all Bureau columns\n",
      "\n",
      "================================================================================\n",
      "IDENTIFYING BUREAU COLUMNS\n",
      "================================================================================\n",
      "\n",
      "Total bureau-derived columns: 71\n",
      "\n",
      "Column types:\n",
      "  Numerical aggregates (_MIN/_MAX/_MEAN/_SUM): 48\n",
      "  Categorical counts (COUNT_*): 23\n",
      "\n",
      "First 10 bureau columns:\n",
      "  - DAYS_CREDIT_MIN\n",
      "  - DAYS_CREDIT_MAX\n",
      "  - DAYS_CREDIT_MEAN\n",
      "  - DAYS_CREDIT_SUM\n",
      "  - CREDIT_DAY_OVERDUE_MIN\n",
      "  - CREDIT_DAY_OVERDUE_MAX\n",
      "  - CREDIT_DAY_OVERDUE_MEAN\n",
      "  - CREDIT_DAY_OVERDUE_SUM\n",
      "  - DAYS_CREDIT_ENDDATE_MIN\n",
      "  - DAYS_CREDIT_ENDDATE_MAX\n",
      "\n",
      "================================================================================\n",
      "CHECKING FOR MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values in bureau columns:\n",
      "  Found NaN values in 71 bureau columns\n",
      "  Expected: 44,020 NaN per column (new-to-credit applicants)\n",
      "\n",
      "  Sample of columns with NaN:\n",
      "    - DAYS_CREDIT_MIN: 44,020 NaN\n",
      "    - DAYS_CREDIT_MAX: 44,020 NaN\n",
      "    - DAYS_CREDIT_MEAN: 44,020 NaN\n",
      "    - DAYS_CREDIT_SUM: 44,020 NaN\n",
      "    - CREDIT_DAY_OVERDUE_MIN: 44,020 NaN\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CREATE CREDIT HISTORY FLAG\n",
      "================================================================================\n",
      "\n",
      "Created new column: HAS_CREDIT_HISTORY\n",
      "  Type: Boolean (1 = has credit history, 0 = new-to-credit)\n",
      "\n",
      "Distribution:\n",
      "  Has credit history (1): 263,491 (85.69%)\n",
      "  No credit history (0):  44,020 (14.31%)\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FILL MISSING VALUES IN BUREAU COLUMNS\n",
      "================================================================================\n",
      "\n",
      "Filling strategy:\n",
      "  All 71 bureau columns: Fill NaN with 0\n",
      "  Rationale:\n",
      "    - COUNT_* columns: 0 means 'no accounts of this type'\n",
      "    - *_MIN/*_MAX/*_MEAN/*_SUM: 0 means 'no credit activity'\n",
      "    - This correctly represents new-to-credit customers\n",
      "\n",
      "✓ Filled 71 columns with 0\n",
      "✓ Total NaN values remaining: 9090840\n",
      "\n",
      "================================================================================\n",
      "FINAL MERGED DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Merge successful!\n",
      "\n",
      "Final Dataset Specifications:\n",
      "  Rows: 307,511\n",
      "  Columns: 192\n",
      "  Data types: 4 different types\n",
      "\n",
      "Column inventory:\n",
      "  Original Application columns: 120\n",
      "  Bureau aggregate columns: 71\n",
      "  New derived column (HAS_CREDIT_HISTORY): 1\n",
      "  Total: 192\n",
      "\n",
      "Key statistics:\n",
      "  All applicants retained: True\n",
      "  No duplicate SK_ID_CURR: True\n",
      "  No NaN values: False\n",
      "  Default rate (TARGET): 8.07%\n",
      "\n",
      "================================================================================\n",
      "SAMPLE DATA: First 3 Applicants\n",
      "================================================================================\n",
      "\n",
      "Application + Bureau Data (selected columns):\n",
      "   SK_ID_CURR  TARGET  AMT_INCOME_TOTAL NAME_INCOME_TYPE  HAS_CREDIT_HISTORY  DAYS_CREDIT_MIN  COUNT_CREDIT_ACTIVE_ACTIVE  AMT_CREDIT_SUM_SUM\n",
      "0      100002       1          202500.0          Working                True          -1437.0                         2.0          865055.565\n",
      "1      100003       0          270000.0    State servant                True          -2586.0                         1.0         1017400.500\n",
      "2      100004       0           67500.0          Working                True          -1326.0                         0.0          189037.800\n",
      "\n",
      "================================================================================\n",
      "✓ LEFT JOIN COMPLETE - READY FOR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Merged dataset (merged_df) is ready for:\n",
      "  1. Filtering for Lifelong Learning segment\n",
      "  2. Feature engineering and analysis\n",
      "  3. Exploratory Data Analysis (EDA)\n",
      "  4. Credit scorecard model development\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LEFT JOIN: Application Data (LEFT) ← Bureau Data (RIGHT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "print(\"  LEFT table (app_df):      307,511 rows × 120 columns\")\n",
    "print(\"  RIGHT table (bureau_flat): 305,811 rows × 72 columns\")\n",
    "print(\"  Join key: SK_ID_CURR\")\n",
    "print(\"  Join type: LEFT JOIN\")\n",
    "\n",
    "print(\"\\nWhat a LEFT JOIN does:\")\n",
    "print(\"  ✓ Keeps ALL rows from the LEFT table (app_df)\")\n",
    "print(\"  ✓ Adds matching data from the RIGHT table (bureau_flat)\")\n",
    "print(\"  ✓ For non-matching rows: fills with NaN\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTING LEFT JOIN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# LEFT JOIN: Application Data on LEFT, Bureau Data on RIGHT\n",
    "# This keeps ALL applications and adds bureau data where available\n",
    "merged_df = app_df.merge(bureau_flat, on='SK_ID_CURR', how='left')\n",
    "\n",
    "print(f\"\\n✓ Merge complete!\")\n",
    "print(f\"\\nResult Dataset:\")\n",
    "print(f\"  Rows: {len(merged_df):,}\")\n",
    "print(f\"  Columns: {len(merged_df.columns)}\")\n",
    "print(f\"\\nColumn breakdown:\")\n",
    "print(f\"  Application columns: {len(app_df.columns)}\")\n",
    "print(f\"  Bureau columns (from right): {len(bureau_flat.columns) - 1} (SK_ID_CURR not duplicated)\")\n",
    "print(f\"  Total: {len(merged_df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGE RESULT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check how many applicants have matching bureau data\n",
    "applicants_with_bureau = merged_df['SK_ID_CURR'].isin(bureau_flat['SK_ID_CURR']).sum()\n",
    "applicants_without_bureau = len(merged_df) - applicants_with_bureau\n",
    "\n",
    "print(f\"\\nApplicants with Bureau data:\")\n",
    "print(f\"  Count: {applicants_with_bureau:,}\")\n",
    "print(f\"  Percentage: {100 * applicants_with_bureau / len(merged_df):.2f}%\")\n",
    "\n",
    "print(f\"\\nApplicants WITHOUT Bureau data (new-to-credit):\")\n",
    "print(f\"  Count: {applicants_without_bureau:,}\")\n",
    "print(f\"  Percentage: {100 * applicants_without_bureau / len(merged_df):.2f}%\")\n",
    "print(f\"  Status: Will have NaN values in all Bureau columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFYING BUREAU COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify all bureau-derived columns\n",
    "bureau_columns = [col for col in merged_df.columns \n",
    "                  if any(suffix in col for suffix in ['_MIN', '_MAX', '_MEAN', '_SUM']) \n",
    "                  or col.startswith('COUNT_')]\n",
    "\n",
    "print(f\"\\nTotal bureau-derived columns: {len(bureau_columns)}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "numerical_agg_cols = [col for col in bureau_columns if any(s in col for s in ['_MIN', '_MAX', '_MEAN', '_SUM'])]\n",
    "categorical_count_cols = [col for col in bureau_columns if col.startswith('COUNT_')]\n",
    "print(f\"  Numerical aggregates (_MIN/_MAX/_MEAN/_SUM): {len(numerical_agg_cols)}\")\n",
    "print(f\"  Categorical counts (COUNT_*): {len(categorical_count_cols)}\")\n",
    "\n",
    "print(f\"\\nFirst 10 bureau columns:\")\n",
    "for col in bureau_columns[:10]:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING FOR MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check NaN values in bureau columns\n",
    "print(f\"\\nMissing values in bureau columns:\")\n",
    "nan_counts = merged_df[bureau_columns].isna().sum()\n",
    "cols_with_nan = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(cols_with_nan) > 0:\n",
    "    print(f\"  Found NaN values in {len(cols_with_nan)} bureau columns\")\n",
    "    print(f\"  Expected: {applicants_without_bureau:,} NaN per column (new-to-credit applicants)\")\n",
    "    print(f\"\\n  Sample of columns with NaN:\")\n",
    "    for col in list(cols_with_nan.head(5).index):\n",
    "        print(f\"    - {col}: {cols_with_nan[col]:,} NaN\")\n",
    "else:\n",
    "    print(f\"  No NaN values found (unexpected!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: CREATE CREDIT HISTORY FLAG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a flag for customers with credit history\n",
    "# Use DAYS_CREDIT_MIN as indicator (it will be NaN only for those without bureau data)\n",
    "merged_df['HAS_CREDIT_HISTORY'] = ~merged_df['DAYS_CREDIT_MIN'].isna()\n",
    "\n",
    "has_history_count = merged_df['HAS_CREDIT_HISTORY'].sum()\n",
    "no_history_count = (~merged_df['HAS_CREDIT_HISTORY']).sum()\n",
    "\n",
    "print(f\"\\nCreated new column: HAS_CREDIT_HISTORY\")\n",
    "print(f\"  Type: Boolean (1 = has credit history, 0 = new-to-credit)\")\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  Has credit history (1): {has_history_count:,} ({100*has_history_count/len(merged_df):.2f}%)\")\n",
    "print(f\"  No credit history (0):  {no_history_count:,} ({100*no_history_count/len(merged_df):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: FILL MISSING VALUES IN BUREAU COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFilling strategy:\")\n",
    "print(f\"  All {len(bureau_columns)} bureau columns: Fill NaN with 0\")\n",
    "print(f\"  Rationale:\")\n",
    "print(f\"    - COUNT_* columns: 0 means 'no accounts of this type'\")\n",
    "print(f\"    - *_MIN/*_MAX/*_MEAN/*_SUM: 0 means 'no credit activity'\")\n",
    "print(f\"    - This correctly represents new-to-credit customers\")\n",
    "\n",
    "# Fill missing values with 0\n",
    "merged_df[bureau_columns] = merged_df[bureau_columns].fillna(0)\n",
    "\n",
    "print(f\"\\n✓ Filled {len(bureau_columns)} columns with 0\")\n",
    "print(f\"✓ Total NaN values remaining: {merged_df.isna().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MERGED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ Merge successful!\")\n",
    "print(f\"\\nFinal Dataset Specifications:\")\n",
    "print(f\"  Rows: {len(merged_df):,}\")\n",
    "print(f\"  Columns: {len(merged_df.columns)}\")\n",
    "print(f\"  Data types: {merged_df.dtypes.nunique()} different types\")\n",
    "\n",
    "print(f\"\\nColumn inventory:\")\n",
    "print(f\"  Original Application columns: {len(app_df.columns)}\")\n",
    "print(f\"  Bureau aggregate columns: {len(bureau_columns)}\")\n",
    "print(f\"  New derived column (HAS_CREDIT_HISTORY): 1\")\n",
    "print(f\"  Total: {len(merged_df.columns)}\")\n",
    "\n",
    "print(f\"\\nKey statistics:\")\n",
    "print(f\"  All applicants retained: {len(merged_df) == len(app_df)}\")\n",
    "print(f\"  No duplicate SK_ID_CURR: {merged_df['SK_ID_CURR'].nunique() == len(merged_df)}\")\n",
    "print(f\"  No NaN values: {merged_df.isna().sum().sum() == 0}\")\n",
    "print(f\"  Default rate (TARGET): {merged_df['TARGET'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DATA: First 3 Applicants\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show sample data with key columns\n",
    "sample_cols = ['SK_ID_CURR', 'TARGET', 'AMT_INCOME_TOTAL', 'NAME_INCOME_TYPE', \n",
    "               'HAS_CREDIT_HISTORY', 'DAYS_CREDIT_MIN', 'COUNT_CREDIT_ACTIVE_ACTIVE', \n",
    "               'AMT_CREDIT_SUM_SUM']\n",
    "sample_cols_present = [col for col in sample_cols if col in merged_df.columns]\n",
    "\n",
    "print(\"\\nApplication + Bureau Data (selected columns):\")\n",
    "print(merged_df[sample_cols_present].head(3).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ LEFT JOIN COMPLETE - READY FOR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMerged dataset (merged_df) is ready for:\")\n",
    "print(f\"  1. Filtering for Lifelong Learning segment\")\n",
    "print(f\"  2. Feature engineering and analysis\")\n",
    "print(f\"  3. Exploratory Data Analysis (EDA)\")\n",
    "print(f\"  4. Credit scorecard model development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ne29kjkhi1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPORTING MERGED DATASET TO CSV\n",
      "================================================================================\n",
      "\n",
      "✓ Exported merged dataset to: IS453_Group_Assignment_Merged_Data.csv\n",
      "\n",
      "Dataset Details:\n",
      "  Rows: 307,511\n",
      "  Columns: 192\n",
      "  File size (on disk): ~698.87 MB\n",
      "\n",
      "Column Summary:\n",
      "  Application columns: 120\n",
      "  Bureau columns: 71 (from flattened data)\n",
      "  Derived columns: 1 (HAS_CREDIT_HISTORY)\n",
      "  Total columns: 192\n",
      "\n",
      "Data Coverage:\n",
      "  Applicants with bureau data: 263,491 (85.7%)\n",
      "  New-to-credit applicants: 44,020 (14.3%)\n",
      "  All applicants retained: YES ✓\n",
      "\n",
      "✓ File saved successfully in current working directory!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING MERGED DATASET TO CSV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = 'IS453_Group_Assignment_Merged_Data.csv'\n",
    "\n",
    "# Guard: Check if file already exists\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"\\n⚠ WARNING: File '{output_file}' already exists!\")\n",
    "    print(f\"\\nOptions:\")\n",
    "    print(f\"  1. Skip export (SAFE - default)\")\n",
    "    print(f\"  2. Overwrite (uncomment line below and re-run)\")\n",
    "    print(f\"  3. Create timestamped version (uncomment alternative below)\")\n",
    "    print(f\"\\nTo OVERWRITE, uncomment the line below:\")\n",
    "    print(f\"  # merged_df.to_csv(output_file, index=False)\")\n",
    "    print(f\"\\nTo CREATE TIMESTAMPED FILE, uncomment:\")\n",
    "    print(f\"  # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\")\n",
    "    print(f\"  # output_file = f'IS453_Group_Assignment_Merged_Data_{timestamp}.csv'\")\n",
    "    print(f\"  # merged_df.to_csv(output_file, index=False)\")\n",
    "    print(f\"\\n✓ Export skipped to prevent accidental overwrite\")\n",
    "    \n",
    "else:\n",
    "    # File doesn't exist, safe to export\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    file_size_mb = merged_df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    \n",
    "    print(f\"\\n✓ Exported merged dataset to: {output_file}\")\n",
    "    print(f\"\\nDataset Details:\")\n",
    "    print(f\"  Rows: {len(merged_df):,}\")\n",
    "    print(f\"  Columns: {len(merged_df.columns)}\")\n",
    "    print(f\"  File size (on disk): ~{file_size_mb:.2f} MB\")\n",
    "    print(f\"\\nColumn Summary:\")\n",
    "    print(f\"  Application columns: {len(app_df.columns)}\")\n",
    "    print(f\"  Bureau columns: 71 (from flattened data)\")\n",
    "    print(f\"  Derived columns: 1 (HAS_CREDIT_HISTORY)\")\n",
    "    print(f\"  Total columns: {len(merged_df.columns)}\")\n",
    "    print(f\"\\nData Coverage:\")\n",
    "    print(f\"  Applicants with bureau data: 263,491 (85.7%)\")\n",
    "    print(f\"  New-to-credit applicants: 44,020 (14.3%)\")\n",
    "    print(f\"  All applicants retained: YES ✓\")\n",
    "    print(f\"\\n✓ File saved successfully in current working directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51f35b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 192)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "r4ixkoe3tg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILTERING MERGED DATA FOR LIFELONG LEARNING LOAN SEGMENT\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL MERGED DATASET:\n",
      "  Total records: 307,511\n",
      "  Columns: 192\n",
      "  Goods (TARGET=0): 282,686\n",
      "  Bads (TARGET=1): 24,825\n",
      "  Bad rate: 8.07%\n",
      "\n",
      "Bureau data coverage:\n",
      "  With bureau history: 263,491\n",
      "  New-to-credit: 44,020\n",
      "\n",
      "================================================================================\n",
      "APPLYING FILTERING CRITERIA TO MERGED DATA\n",
      "================================================================================\n",
      "\n",
      "FILTERED SEGMENT (Lifelong Learning Loans):\n",
      "  Total records: 31,125\n",
      "  Columns: 192\n",
      "  Goods (TARGET=0): 28,016\n",
      "  Bads (TARGET=1): 3,109\n",
      "  Bad rate: 9.99%\n",
      "\n",
      "Bureau data in filtered segment:\n",
      "  With bureau history: 25,380 (81.54%)\n",
      "  New-to-credit: 5,745 (18.46%)\n",
      "\n",
      "================================================================================\n",
      "COMPARISON METRICS\n",
      "================================================================================\n",
      "\n",
      "RETENTION:\n",
      "  Records retained: 31,125 out of 307,511\n",
      "  Retention rate: 10.12%\n",
      "\n",
      "BAD RATE COMPARISON:\n",
      "  Original merged data: 8.07%\n",
      "  Filtered segment: 9.99%\n",
      "  Difference: +1.92 percentage points\n",
      "  → Target segment is HIGHER RISK (worse than average)\n",
      "\n",
      "================================================================================\n",
      "BREAKDOWN BY INDIVIDUAL CRITERIA\n",
      "================================================================================\n",
      "\n",
      "How each criterion narrows the merged dataset:\n",
      "\n",
      "1. Age (25-55 years)\n",
      "  Records: 226,662 (73.71%)\n",
      "  Bad rate: 8.71%\n",
      "\n",
      "2. Income Type (Working/Commercial/State)\n",
      "  Records: 252,121 (81.99%)\n",
      "  Bad rate: 8.66%\n",
      "\n",
      "3. Contract Type (Cash loans)\n",
      "  Records: 278,232 (90.48%)\n",
      "  Bad rate: 8.35%\n",
      "\n",
      "4. Income (≤ $96,000)\n",
      "  Records: 59,879 (19.47%)\n",
      "  Bad rate: 8.17%\n",
      "\n",
      "================================================================================\n",
      "FILTERED SEGMENT SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Filtering Complete!\n",
      "\n",
      "ORIGINAL MERGED DATASET:\n",
      "  - Records: 307,511\n",
      "  - Columns: 192 (120 app + 71 bureau + 1 flag)\n",
      "\n",
      "FILTERED SEGMENT (Lifelong Learning):\n",
      "  - Records: 31,125 (10.12% retention)\n",
      "  - Columns: 192 (same as original)\n",
      "  - Default rate: 9.99% (+1.92 pp vs original)\n",
      "\n",
      "BUREAU DATA AVAILABILITY:\n",
      "  - Applicants with bureau history: 25,380\n",
      "  - New-to-credit applicants: 5,745\n",
      "  - Ready for analysis with 71 bureau features!\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Use 'filtered_merged' for Lifelong Learning segment analysis\n",
      "  2. Analyze bureau features impact on default rate\n",
      "  3. Build credit scorecard using merged data with bureau aggregates\n",
      "  4. Create feature engineering from bureau data\n",
      "\n",
      "Variable name: filtered_merged\n",
      "Shape: (31125, 192)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FILTERING MERGED DATA FOR LIFELONG LEARNING LOAN SEGMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store original dataset metrics (using merged_df)\n",
    "total_original = len(merged_df)\n",
    "original_bad_count = merged_df['TARGET'].sum()\n",
    "original_bad_rate = merged_df['TARGET'].mean() * 100\n",
    "\n",
    "print(f\"\\nORIGINAL MERGED DATASET:\")\n",
    "print(f\"  Total records: {total_original:,}\")\n",
    "print(f\"  Columns: {len(merged_df.columns)}\")\n",
    "print(f\"  Goods (TARGET=0): {(merged_df['TARGET']==0).sum():,}\")\n",
    "print(f\"  Bads (TARGET=1): {original_bad_count:,}\")\n",
    "print(f\"  Bad rate: {original_bad_rate:.2f}%\")\n",
    "print(f\"\\nBureau data coverage:\")\n",
    "print(f\"  With bureau history: {merged_df['HAS_CREDIT_HISTORY'].sum():,}\")\n",
    "print(f\"  New-to-credit: {(~merged_df['HAS_CREDIT_HISTORY']).sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING FILTERING CRITERIA TO MERGED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply all 5 criteria simultaneously to merged_df\n",
    "filtered_merged = merged_df[\n",
    "    (merged_df['DAYS_BIRTH'] >= -20075) & \n",
    "    (merged_df['DAYS_BIRTH'] <= -9125) &\n",
    "    (merged_df['AMT_INCOME_TOTAL'] <= 96000) &\n",
    "    (merged_df['NAME_CONTRACT_TYPE'] == 'Cash loans') &\n",
    "    (merged_df['NAME_INCOME_TYPE'].isin(['Working', 'Commercial associate', 'State servant','Unemployed','Maternity Leave']))\n",
    "]\n",
    "\n",
    "# Calculate filtered dataset metrics\n",
    "total_filtered = len(filtered_merged)\n",
    "filtered_bad_count = filtered_merged['TARGET'].sum()\n",
    "filtered_bad_rate = filtered_merged['TARGET'].mean() * 100\n",
    "\n",
    "percentage_retained = (total_filtered / total_original) * 100\n",
    "\n",
    "print(\"\\nFILTERED SEGMENT (Lifelong Learning Loans):\")\n",
    "print(f\"  Total records: {total_filtered:,}\")\n",
    "print(f\"  Columns: {len(filtered_merged.columns)}\")\n",
    "print(f\"  Goods (TARGET=0): {(filtered_merged['TARGET']==0).sum():,}\")\n",
    "print(f\"  Bads (TARGET=1): {filtered_bad_count:,}\")\n",
    "print(f\"  Bad rate: {filtered_bad_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nBureau data in filtered segment:\")\n",
    "print(f\"  With bureau history: {filtered_merged['HAS_CREDIT_HISTORY'].sum():,} ({100*filtered_merged['HAS_CREDIT_HISTORY'].sum()/len(filtered_merged):.2f}%)\")\n",
    "print(f\"  New-to-credit: {(~filtered_merged['HAS_CREDIT_HISTORY']).sum():,} ({100*(~filtered_merged['HAS_CREDIT_HISTORY']).sum()/len(filtered_merged):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate differences\n",
    "bad_rate_difference = filtered_bad_rate - original_bad_rate\n",
    "retention_rate = percentage_retained\n",
    "\n",
    "print(f\"\\nRETENTION:\")\n",
    "print(f\"  Records retained: {total_filtered:,} out of {total_original:,}\")\n",
    "print(f\"  Retention rate: {retention_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nBAD RATE COMPARISON:\")\n",
    "print(f\"  Original merged data: {original_bad_rate:.2f}%\")\n",
    "print(f\"  Filtered segment: {filtered_bad_rate:.2f}%\")\n",
    "print(f\"  Difference: {bad_rate_difference:+.2f} percentage points\")\n",
    "\n",
    "if bad_rate_difference < 0:\n",
    "    print(f\"  → Target segment is LOWER RISK (better than average)\")\n",
    "elif bad_rate_difference > 0:\n",
    "    print(f\"  → Target segment is HIGHER RISK (worse than average)\")\n",
    "else:\n",
    "    print(f\"  → Target segment has SIMILAR RISK to overall portfolio\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BREAKDOWN BY INDIVIDUAL CRITERIA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show impact of each filter\n",
    "print(\"\\nHow each criterion narrows the merged dataset:\")\n",
    "\n",
    "criteria = [\n",
    "    ('1. Age (25-55 years)', \n",
    "     merged_df[(merged_df['DAYS_BIRTH'] >= -20075) & (merged_df['DAYS_BIRTH'] <= -9125)]),\n",
    "    ('2. Income Type (Working/Commercial/State)', \n",
    "     merged_df[merged_df['NAME_INCOME_TYPE'].isin(['Working', 'Commercial associate', 'State servant','Unemployed', 'Maternity leave'])]),\n",
    "    ('3. Contract Type (Cash loans)', \n",
    "     merged_df[merged_df['NAME_CONTRACT_TYPE'] == 'Cash loans']),\n",
    "    ('4. Income (≤ $96,000)', \n",
    "     merged_df[merged_df['AMT_INCOME_TOTAL'] <= 96000])\n",
    "]\n",
    "\n",
    "for name, subset in criteria:\n",
    "    count = len(subset)\n",
    "    pct = (count / total_original) * 100\n",
    "    bad_rate = subset['TARGET'].mean() * 100\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Records: {count:,} ({pct:.2f}%)\")\n",
    "    print(f\"  Bad rate: {bad_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILTERED SEGMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "✓ Filtering Complete!\n",
    "\n",
    "ORIGINAL MERGED DATASET:\n",
    "  - Records: {total_original:,}\n",
    "  - Columns: {len(merged_df.columns)} (120 app + 71 bureau + 1 flag)\n",
    "\n",
    "FILTERED SEGMENT (Lifelong Learning):\n",
    "  - Records: {total_filtered:,} ({retention_rate:.2f}% retention)\n",
    "  - Columns: {len(filtered_merged.columns)} (same as original)\n",
    "  - Default rate: {filtered_bad_rate:.2f}% ({bad_rate_difference:+.2f} pp vs original)\n",
    "\n",
    "BUREAU DATA AVAILABILITY:\n",
    "  - Applicants with bureau history: {filtered_merged['HAS_CREDIT_HISTORY'].sum():,}\n",
    "  - New-to-credit applicants: {(~filtered_merged['HAS_CREDIT_HISTORY']).sum():,}\n",
    "  - Ready for analysis with {len([c for c in filtered_merged.columns if any(s in c for s in ['_MIN', '_MAX', '_MEAN', '_SUM']) or c.startswith('COUNT_')])} bureau features!\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Use 'filtered_merged' for Lifelong Learning segment analysis\n",
    "  2. Analyze bureau features impact on default rate\n",
    "  3. Build credit scorecard using merged data with bureau aggregates\n",
    "  4. Create feature engineering from bureau data\n",
    "\"\"\")\n",
    "\n",
    "print(\"Variable name: filtered_merged\")\n",
    "print(f\"Shape: {filtered_merged.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is453",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
